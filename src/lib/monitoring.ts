import { createClient } from '@/lib/supabase/server'

export interface ErrorEvent {
  type: 'api_error' | 'webhook_error' | 'bulk_operation_error' | 'system_error'
  message: string
  stack?: string
  code?: string
  userId?: string
  organizationId?: string
  endpoint?: string
  metadata?: Record<string, any>
  severity: 'low' | 'medium' | 'high' | 'critical'
  timestamp: string
}

export interface PerformanceMetric {
  endpoint: string
  method: string
  duration: number
  statusCode: number
  userId?: string
  organizationId?: string
  timestamp: string
}

export interface SystemAlert {
  type: 'rate_limit_exceeded' | 'high_error_rate' | 'slow_response' | 'quota_exceeded' | 'service_degradation'
  severity: 'warning' | 'error' | 'critical'
  message: string
  organizationId?: string
  metadata?: Record<string, any>
  threshold?: number
  currentValue?: number
  timestamp: string
}

class MonitoringService {
  private supabase = createClient()

  constructor() {
    this.supabase = createClient()
  }

  async logError(error: ErrorEvent): Promise<void> {
    try {
      const supabase = await this.supabase

      await supabase
        .from('error_logs')
        .insert({
          type: error.type,
          message: error.message,
          stack: error.stack,
          code: error.code,
          user_id: error.userId,
          organization_id: error.organizationId,
          endpoint: error.endpoint,
          metadata: error.metadata,
          severity: error.severity,
          timestamp: error.timestamp
        })

      // Create alert for high severity errors
      if (error.severity === 'high' || error.severity === 'critical') {
        await this.createAlert({
          type: 'high_error_rate',
          severity: error.severity === 'critical' ? 'critical' : 'error',
          message: `${error.severity.toUpperCase()} error: ${error.message}`,
          organizationId: error.organizationId,
          metadata: {
            errorCode: error.code,
            endpoint: error.endpoint,
            originalError: error.message
          },
          timestamp: new Date().toISOString()
        })
      }

    } catch (logError) {
      console.error('Failed to log error to monitoring service:', logError)
    }
  }

  async logPerformance(metric: PerformanceMetric): Promise<void> {
    try {
      const supabase = await this.supabase

      await supabase
        .from('performance_metrics')
        .insert({
          endpoint: metric.endpoint,
          method: metric.method,
          duration_ms: metric.duration,
          status_code: metric.statusCode,
          user_id: metric.userId,
          organization_id: metric.organizationId,
          timestamp: metric.timestamp
        })

      // Alert on slow responses
      if (metric.duration > 5000) { // 5 seconds
        await this.createAlert({
          type: 'slow_response',
          severity: metric.duration > 10000 ? 'error' : 'warning',
          message: `Slow API response detected: ${metric.endpoint} took ${metric.duration}ms`,
          organizationId: metric.organizationId,
          metadata: {
            endpoint: metric.endpoint,
            method: metric.method,
            duration: metric.duration,
            statusCode: metric.statusCode
          },
          threshold: 5000,
          currentValue: metric.duration,
          timestamp: new Date().toISOString()
        })
      }

    } catch (logError) {
      console.error('Failed to log performance metric:', logError)
    }
  }

  async createAlert(alert: SystemAlert): Promise<void> {
    try {
      const supabase = await this.supabase

      // Check if similar alert was created recently (avoid spam)
      const oneHourAgo = new Date(Date.now() - 60 * 60 * 1000).toISOString()
      const { data: recentAlert } = await supabase
        .from('system_alerts')
        .select('id')
        .eq('type', alert.type)
        .eq('organization_id', alert.organizationId || null)
        .gte('created_at', oneHourAgo)
        .limit(1)
        .single()

      if (recentAlert) {
        // Update existing alert instead of creating new one
        await supabase
          .from('system_alerts')
          .update({
            current_value: alert.currentValue,
            updated_at: new Date().toISOString(),
            occurrence_count: supabase.sql`occurrence_count + 1`
          })
          .eq('id', recentAlert.id)

        return
      }

      // Create new alert
      await supabase
        .from('system_alerts')
        .insert({
          type: alert.type,
          severity: alert.severity,
          message: alert.message,
          organization_id: alert.organizationId,
          metadata: alert.metadata,
          threshold: alert.threshold,
          current_value: alert.currentValue,
          occurrence_count: 1,
          is_resolved: false,
          created_at: alert.timestamp
        })

      // For critical alerts, also log to external monitoring if configured
      if (alert.severity === 'critical') {
        await this.notifyExternalMonitoring(alert)
      }

    } catch (logError) {
      console.error('Failed to create system alert:', logError)
    }
  }

  async getHealthMetrics(organizationId?: string): Promise<{
    errorRate: number
    averageResponseTime: number
    activeAlerts: number
    uptime: number
  }> {
    try {
      const supabase = await this.supabase
      const oneHourAgo = new Date(Date.now() - 60 * 60 * 1000).toISOString()

      // Get error rate (last hour)
      let errorQuery = supabase
        .from('error_logs')
        .select('id', { count: 'exact' })
        .gte('timestamp', oneHourAgo)

      let performanceQuery = supabase
        .from('performance_metrics')
        .select('duration_ms')
        .gte('timestamp', oneHourAgo)

      let alertQuery = supabase
        .from('system_alerts')
        .select('id', { count: 'exact' })
        .eq('is_resolved', false)

      if (organizationId) {
        errorQuery = errorQuery.eq('organization_id', organizationId)
        performanceQuery = performanceQuery.eq('organization_id', organizationId)
        alertQuery = alertQuery.eq('organization_id', organizationId)
      }

      const [errorResult, performanceResult, alertResult] = await Promise.all([
        errorQuery,
        performanceQuery,
        alertQuery
      ])

      const errorCount = errorResult.count || 0
      const performanceData = performanceResult.data || []
      const alertCount = alertResult.count || 0

      const averageResponseTime = performanceData.length > 0
        ? performanceData.reduce((sum, metric) => sum + metric.duration_ms, 0) / performanceData.length
        : 0

      // Calculate error rate as percentage
      const totalRequests = performanceData.length
      const errorRate = totalRequests > 0 ? (errorCount / totalRequests) * 100 : 0

      // Simple uptime calculation (percentage of successful requests)
      const uptime = totalRequests > 0 ? Math.max(0, 100 - errorRate) : 100

      return {
        errorRate: Math.round(errorRate * 100) / 100,
        averageResponseTime: Math.round(averageResponseTime),
        activeAlerts: alertCount,
        uptime: Math.round(uptime * 100) / 100
      }

    } catch (error) {
      console.error('Failed to get health metrics:', error)
      return {
        errorRate: 0,
        averageResponseTime: 0,
        activeAlerts: 0,
        uptime: 100
      }
    }
  }

  async resolveAlert(alertId: string): Promise<void> {
    try {
      const supabase = await this.supabase

      await supabase
        .from('system_alerts')
        .update({
          is_resolved: true,
          resolved_at: new Date().toISOString()
        })
        .eq('id', alertId)

    } catch (error) {
      console.error('Failed to resolve alert:', error)
    }
  }

  async getErrorTrends(organizationId?: string, days: number = 7): Promise<Array<{
    date: string
    errorCount: number
    avgResponseTime: number
    totalRequests: number
  }>> {
    try {
      const supabase = await this.supabase
      const trends = []

      for (let i = days - 1; i >= 0; i--) {
        const date = new Date(Date.now() - i * 24 * 60 * 60 * 1000)
        const dateStr = date.toISOString().split('T')[0]
        const startOfDay = `${dateStr}T00:00:00.000Z`
        const endOfDay = `${dateStr}T23:59:59.999Z`

        let errorQuery = supabase
          .from('error_logs')
          .select('id', { count: 'exact' })
          .gte('timestamp', startOfDay)
          .lte('timestamp', endOfDay)

        let performanceQuery = supabase
          .from('performance_metrics')
          .select('duration_ms')
          .gte('timestamp', startOfDay)
          .lte('timestamp', endOfDay)

        if (organizationId) {
          errorQuery = errorQuery.eq('organization_id', organizationId)
          performanceQuery = performanceQuery.eq('organization_id', organizationId)
        }

        const [errorResult, performanceResult] = await Promise.all([
          errorQuery,
          performanceQuery
        ])

        const errorCount = errorResult.count || 0
        const performanceData = performanceResult.data || []
        const avgResponseTime = performanceData.length > 0
          ? performanceData.reduce((sum, metric) => sum + metric.duration_ms, 0) / performanceData.length
          : 0

        trends.push({
          date: dateStr,
          errorCount,
          avgResponseTime: Math.round(avgResponseTime),
          totalRequests: performanceData.length
        })
      }

      return trends

    } catch (error) {
      console.error('Failed to get error trends:', error)
      return []
    }
  }

  private async notifyExternalMonitoring(alert: SystemAlert): Promise<void> {
    try {
      // Integration with external monitoring services
      // This could be Sentry, DataDog, New Relic, etc.

      if (process.env.SENTRY_DSN) {
        // Send to Sentry
        console.error('CRITICAL ALERT:', alert)
      }

      if (process.env.SLACK_WEBHOOK_URL) {
        // Send to Slack
        await fetch(process.env.SLACK_WEBHOOK_URL, {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            text: `🚨 CRITICAL ALERT: ${alert.message}`,
            attachments: [{
              color: 'danger',
              fields: [
                { title: 'Type', value: alert.type, short: true },
                { title: 'Severity', value: alert.severity, short: true },
                { title: 'Organization', value: alert.organizationId || 'System', short: true },
                { title: 'Timestamp', value: alert.timestamp, short: true }
              ]
            }]
          })
        })
      }

    } catch (error) {
      console.error('Failed to notify external monitoring:', error)
    }
  }
}

// Export singleton instance
export const monitoring = new MonitoringService()

// Convenience functions
export async function logError(error: Partial<ErrorEvent>): Promise<void> {
  await monitoring.logError({
    type: 'system_error',
    severity: 'medium',
    timestamp: new Date().toISOString(),
    ...error
  } as ErrorEvent)
}

export async function logApiError(
  message: string,
  endpoint: string,
  userId?: string,
  organizationId?: string,
  metadata?: Record<string, any>
): Promise<void> {
  await monitoring.logError({
    type: 'api_error',
    message,
    endpoint,
    userId,
    organizationId,
    metadata,
    severity: 'medium',
    timestamp: new Date().toISOString()
  })
}

export async function logPerformance(
  endpoint: string,
  method: string,
  duration: number,
  statusCode: number,
  userId?: string,
  organizationId?: string
): Promise<void> {
  await monitoring.logPerformance({
    endpoint,
    method,
    duration,
    statusCode,
    userId,
    organizationId,
    timestamp: new Date().toISOString()
  })
}

export async function createAlert(alert: Partial<SystemAlert>): Promise<void> {
  await monitoring.createAlert({
    severity: 'warning',
    timestamp: new Date().toISOString(),
    ...alert
  } as SystemAlert)
}